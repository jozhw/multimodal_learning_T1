# Code to i) create files containing data mapping information (for all modes of training), and ii) joint fusion
# Note: for training using early fusion, refer to early_fusion_survival.py

import pandas as pd
import torch
import h5py
import numpy as np

import os
import argparse

from torch.profiler import profile, record_function, ProfilerActivity
from PIL import Image
from sklearn.model_selection import train_test_split

from .train_observ_test import train_observ_test
from .train import train_nn

from joint_fusion.config.config_manager import ConfigManager
from joint_fusion.utils.logging import setup_logging
import logging
import warnings


def args():

    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config",
        type=str,
        default="/lus/eagle/clone/g2/projects/GeomicVar/jozhw/multimodal_learning_T1/joint_fusion/config/base_config.yaml",
    )

    return parser.parse_args()


def create_h5_file(file_name, train_df, val_df, test_df, image_dir):
    with h5py.File(file_name, "w") as hdf:
        for df, split in zip([train_df, val_df, test_df], ["train", "val", "test"]):
            split_group = hdf.create_group(split)
            total_rows = len(df)
            count_row = 0
            for idx, row in df.iterrows():
                # set_trace()
                count_row += 1
                print(f"Processing {split} data: {count_row}/{total_rows} rows")
                patient_group = split_group.create_group(idx)
                patient_group.create_dataset("days_to_death", data=row["days_to_death"])
                patient_group.create_dataset(
                    "days_to_last_followup", data=row["days_to_last_followup"]
                )
                patient_group.create_dataset("days_to_event", data=row["time"])
                patient_group.create_dataset(
                    "event_occurred", data=1 if row["event_occurred"] == "Dead" else 0
                )

                rnaseq_data = np.array(list(row["rnaseq_data"].values()))
                patient_group.create_dataset("rnaseq_data", data=rnaseq_data)

                # store image tiles
                images_group = patient_group.create_group("images")
                for i, tile in enumerate(row["tiles"]):
                    image_path = os.path.join(image_dir, tile)
                    image = Image.open(image_path)
                    img_arr = np.array(image)
                    images_group.create_dataset(
                        f"image_{i}", data=img_arr, compression="gzip"
                    )


def main():

    opt = args()

    config = ConfigManager.load_config(opt.config)

    os.makedirs(config.logging.checkpoint_dir, exist_ok=True)
    ConfigManager.save_config(config, config.logging.checkpoint_dir)

    logging.captureWarnings(True)

    logger = setup_logging(
        log_dir=config.logging.checkpoint_dir,
        log_level=config.logging.log_level,
    )

    logger.info("Starting training")
    logger.info(f"Checkpoint dir: {config.logging.checkpoint_dir}")

    if config.gpu.gpu_ids and config.gpu.gpu_ids != "-1":
        gpu_list = [int(x) for x in config.gpu.gpu_ids.split(",")]
        device = torch.device(f"cuda:{gpu_list[0]}")
        print(f"Using GPUs: {gpu_list}")
    else:
        device = torch.device("cpu")

    logging.info("Using device:", device)
    torch.backends.cudnn.benchmark = True  # A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.

    if config.data.create_new_data_mapping:
        # create data mappings
        # read the file containing gene expression and tile image locations for the TCGA-LUAD samples (mapped_data_16March)
        # mapping_df = pd.read_csv(opt.input_path + "mapped_data_21March.csv")

        # mapping_df = pd.read_json(opt.input_path + "mapped_data_8may.json", orient='index') # file generated by create_image_molecular_mapping.py
        mapping_df = pd.read_json(
            config.data.input_path + "mapped_data_23july.json", orient="index"
        )
        logging.info("Total number of samples: ", mapping_df.shape[0])
        ids_with_wsi = mapping_df[mapping_df["tiles"].map(len) > 0].index.tolist()
        rnaseq_df = pd.DataFrame(
            mapping_df["rnaseq_data"].to_list(), index=mapping_df.index
        ).transpose()
        logging.warning("Are there nans in rnaseq_df: ", rnaseq_df.isna().any().any())
        # df containing entries where both WSI and rnaseq data are available
        mapping_df = mapping_df.loc[ids_with_wsi]
        logging.info(
            "Total number of samples where both rnaseq and wsi data are available: ",
            mapping_df.shape[0],
        )
        # set_trace()
        # remove rows where the number of tiles is different from the standard (to avoid length mismatch issues during batching)
        # edit it to keep only 200 random slides among all the available ones
        mask = mapping_df["tiles"].apply(len) == 200
        mapping_df = mapping_df[mask]

        mapping_df["time"] = mapping_df["days_to_death"].fillna(
            mapping_df["days_to_last_followup"]
        )
        mapping_df = mapping_df.dropna(subset=["time", "event_occurred"])
        # remove that from the wsi and rnaseq combined embeddings too
        # rnaseq_df = rnaseq_df.drop(columns=['TCGA-49-6742'])
        # remove entries with anomalous time_to_death and 'days_to_last_followup' data
        excluded_ids = [
            "TCGA-05-4395",
            "TCGA-86-8281",
        ]  # contains anomalous time to event and censoring data
        mapping_df = mapping_df[~mapping_df.index.isin(excluded_ids)]
        mapping_df.to_json("mapping_df.json", orient="index")
        rnaseq_df.to_json("rnaseq_df.json", orient="index")
    else:
        mapping_df = pd.read_json(
            config.data.input_mapping_data_path + "mapping_df.json", orient="index"
        )
        # remove entries with anomalous time_to_death and 'days_to_last_followup' data
        excluded_ids = [
            "TCGA-05-4395",
            "TCGA-86-8281",
        ]  # contains anomalous time to event and censoring data
        mapping_df = mapping_df[~mapping_df.index.isin(excluded_ids)]

    # set_trace()

    mapping_df_train, temp_df = train_test_split(
        mapping_df, test_size=0.3, random_state=40
    )
    mapping_df_val, mapping_df_test = train_test_split(
        temp_df, test_size=0.5, random_state=40
    )

    if config.data.create_new_data_mapping_h5:
        # create h5 version of mapping_df for faster IO
        create_h5_file(
            "joint_fusion/mapping_data.h5",
            mapping_df_train,
            mapping_df_val,
            mapping_df_test,
            config.data.input_wsi_path,
        )

    if not config.data.only_create_new_data_mapping:

        # train the model
        if config.logging.profile:
            with profile(
                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                record_shapes=True,
                profile_memory=True,
            ) as prof:
                with record_function("model_train"):
                    model, optimizer = train_nn(
                        config, "joint_fusion/mapping_data.h5", device
                    )
            logging.info("Finishing profiling...")
            logging.info(
                prof.key_averages().table(sort_by="cuda_time_total", row_limit=10)
            )
            trace_path = os.path.join(config.logging.checkpoint_dir, "trace.json")
            prof.export_chrome_trace(trace_path)
            logging.info(f"Saved profile at {trace_path}")

        else:
            model, optimizer = train_nn(config, config.data.h5_file, device)
            # used to observe test set to see if training is stable
            # model, optimizer = train_observ_test(config, config.data.h5_file, device)


if __name__ == "__main__":

    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    main()
